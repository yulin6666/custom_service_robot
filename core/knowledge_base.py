"""
çŸ¥è¯†åº“RAGç³»ç»Ÿ
"""
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from typing import List
from .config import vector_store, KNOWLEDGE_BASE_PATH, TOP_K_RESULTS


class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†ç±»"""

    def __init__(self):
        self.vector_store = vector_store
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " "]
        )
        self.initialized = False

    def load_knowledge_base(self, file_path: str = KNOWLEDGE_BASE_PATH):
        """åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶"""
        try:
            # æ¸…ç©ºæ—§çš„å‘é‡å­˜å‚¨æ•°æ®ï¼ˆé‡è¦ï¼é¿å…æ—§æ•°æ®å¹²æ‰°ï¼‰
            # ç”±äºInMemoryVectorStoreæ²¡æœ‰clearæ–¹æ³•ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åˆ›å»ºå®ä¾‹
            from .config import embeddings
            from langchain_core.vectorstores import InMemoryVectorStore
            self.vector_store = InMemoryVectorStore(embeddings)

            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_text(content)

            # åˆ›å»ºDocumentå¯¹è±¡
            documents = [
                Document(page_content=chunk, metadata={"source": file_path})
                for chunk in chunks
            ]

            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            self.vector_store.add_documents(documents)
            self.initialized = True

            print(f"âœ… æˆåŠŸåŠ è½½çŸ¥è¯†åº“ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£å—")
            print(f"ğŸ“„ çŸ¥è¯†åº“æ–‡ä»¶: {file_path}")
            return True

        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False

    def search(self, query: str, k: int = TOP_K_RESULTS) -> List[Document]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.initialized:
            print("è­¦å‘Š: çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
            return []

        try:
            print(f"\n{'='*60}")
            print(f"[RAGæ£€ç´¢] å¼€å§‹æ£€ç´¢")
            print(f"[RAGæ£€ç´¢] æŸ¥è¯¢: {query}")
            print(f"[RAGæ£€ç´¢] æ£€ç´¢Top-{k}ç»“æœ")

            results = self.vector_store.similarity_search(query, k=k)

            print(f"[RAGæ£€ç´¢] æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            if results:
                for i, doc in enumerate(results, 1):
                    preview = doc.page_content[:100].replace('\n', ' ')
                    print(f"[RAGæ£€ç´¢] æ–‡æ¡£{i}: {preview}...")
            else:
                print(f"[RAGæ£€ç´¢] æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            print(f"{'='*60}\n")

            return results
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []

    def search_with_score(self, query: str, k: int = TOP_K_RESULTS):
        """æœç´¢ç›¸å…³æ–‡æ¡£å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°"""
        if not self.initialized:
            print("è­¦å‘Š: çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
            return []

        try:
            results = self.vector_store.similarity_search_with_score(query, k=k)
            return results
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []


# åˆ›å»ºå…¨å±€çŸ¥è¯†åº“å®ä¾‹
knowledge_base = KnowledgeBase()
